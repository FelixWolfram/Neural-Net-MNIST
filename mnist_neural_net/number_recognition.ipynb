{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "6171c630",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_df = pd.read_csv(\"data/mnist_train.csv\")\n",
    "test_df = pd.read_csv(\"data/mnist_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "244bcc21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>1x1</th>\n",
       "      <th>1x2</th>\n",
       "      <th>1x3</th>\n",
       "      <th>1x4</th>\n",
       "      <th>1x5</th>\n",
       "      <th>1x6</th>\n",
       "      <th>1x7</th>\n",
       "      <th>1x8</th>\n",
       "      <th>1x9</th>\n",
       "      <th>...</th>\n",
       "      <th>28x19</th>\n",
       "      <th>28x20</th>\n",
       "      <th>28x21</th>\n",
       "      <th>28x22</th>\n",
       "      <th>28x23</th>\n",
       "      <th>28x24</th>\n",
       "      <th>28x25</th>\n",
       "      <th>28x26</th>\n",
       "      <th>28x27</th>\n",
       "      <th>28x28</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  1x1  1x2  1x3  1x4  1x5  1x6  1x7  1x8  1x9  ...  28x19  28x20  \\\n",
       "0      5    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "1      0    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "2      4    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "3      1    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "4      9    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "\n",
       "   28x21  28x22  28x23  28x24  28x25  28x26  28x27  28x28  \n",
       "0      0      0      0      0      0      0      0      0  \n",
       "1      0      0      0      0      0      0      0      0  \n",
       "2      0      0      0      0      0      0      0      0  \n",
       "3      0      0      0      0      0      0      0      0  \n",
       "4      0      0      0      0      0      0      0      0  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "44ec129a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>1x1</th>\n",
       "      <th>1x2</th>\n",
       "      <th>1x3</th>\n",
       "      <th>1x4</th>\n",
       "      <th>1x5</th>\n",
       "      <th>1x6</th>\n",
       "      <th>1x7</th>\n",
       "      <th>1x8</th>\n",
       "      <th>1x9</th>\n",
       "      <th>...</th>\n",
       "      <th>28x19</th>\n",
       "      <th>28x20</th>\n",
       "      <th>28x21</th>\n",
       "      <th>28x22</th>\n",
       "      <th>28x23</th>\n",
       "      <th>28x24</th>\n",
       "      <th>28x25</th>\n",
       "      <th>28x26</th>\n",
       "      <th>28x27</th>\n",
       "      <th>28x28</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>60000.000000</td>\n",
       "      <td>60000.0</td>\n",
       "      <td>60000.0</td>\n",
       "      <td>60000.0</td>\n",
       "      <td>60000.0</td>\n",
       "      <td>60000.0</td>\n",
       "      <td>60000.0</td>\n",
       "      <td>60000.0</td>\n",
       "      <td>60000.0</td>\n",
       "      <td>60000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>60000.000000</td>\n",
       "      <td>60000.000000</td>\n",
       "      <td>60000.000000</td>\n",
       "      <td>60000.000000</td>\n",
       "      <td>60000.000000</td>\n",
       "      <td>60000.0000</td>\n",
       "      <td>60000.0</td>\n",
       "      <td>60000.0</td>\n",
       "      <td>60000.0</td>\n",
       "      <td>60000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4.453933</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.200433</td>\n",
       "      <td>0.088867</td>\n",
       "      <td>0.045633</td>\n",
       "      <td>0.019283</td>\n",
       "      <td>0.015117</td>\n",
       "      <td>0.0020</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.889270</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>6.042472</td>\n",
       "      <td>3.956189</td>\n",
       "      <td>2.839845</td>\n",
       "      <td>1.686770</td>\n",
       "      <td>1.678283</td>\n",
       "      <td>0.3466</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>9.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>254.000000</td>\n",
       "      <td>254.000000</td>\n",
       "      <td>253.000000</td>\n",
       "      <td>253.000000</td>\n",
       "      <td>254.000000</td>\n",
       "      <td>62.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              label      1x1      1x2      1x3      1x4      1x5      1x6  \\\n",
       "count  60000.000000  60000.0  60000.0  60000.0  60000.0  60000.0  60000.0   \n",
       "mean       4.453933      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "std        2.889270      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "min        0.000000      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "25%        2.000000      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "50%        4.000000      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "75%        7.000000      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "max        9.000000      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "\n",
       "           1x7      1x8      1x9  ...         28x19         28x20  \\\n",
       "count  60000.0  60000.0  60000.0  ...  60000.000000  60000.000000   \n",
       "mean       0.0      0.0      0.0  ...      0.200433      0.088867   \n",
       "std        0.0      0.0      0.0  ...      6.042472      3.956189   \n",
       "min        0.0      0.0      0.0  ...      0.000000      0.000000   \n",
       "25%        0.0      0.0      0.0  ...      0.000000      0.000000   \n",
       "50%        0.0      0.0      0.0  ...      0.000000      0.000000   \n",
       "75%        0.0      0.0      0.0  ...      0.000000      0.000000   \n",
       "max        0.0      0.0      0.0  ...    254.000000    254.000000   \n",
       "\n",
       "              28x21         28x22         28x23       28x24    28x25    28x26  \\\n",
       "count  60000.000000  60000.000000  60000.000000  60000.0000  60000.0  60000.0   \n",
       "mean       0.045633      0.019283      0.015117      0.0020      0.0      0.0   \n",
       "std        2.839845      1.686770      1.678283      0.3466      0.0      0.0   \n",
       "min        0.000000      0.000000      0.000000      0.0000      0.0      0.0   \n",
       "25%        0.000000      0.000000      0.000000      0.0000      0.0      0.0   \n",
       "50%        0.000000      0.000000      0.000000      0.0000      0.0      0.0   \n",
       "75%        0.000000      0.000000      0.000000      0.0000      0.0      0.0   \n",
       "max      253.000000    253.000000    254.000000     62.0000      0.0      0.0   \n",
       "\n",
       "         28x27    28x28  \n",
       "count  60000.0  60000.0  \n",
       "mean       0.0      0.0  \n",
       "std        0.0      0.0  \n",
       "min        0.0      0.0  \n",
       "25%        0.0      0.0  \n",
       "50%        0.0      0.0  \n",
       "75%        0.0      0.0  \n",
       "max        0.0      0.0  \n",
       "\n",
       "[8 rows x 785 columns]"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "0988868c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 60000 entries, 0 to 59999\n",
      "Columns: 785 entries, label to 28x28\n",
      "dtypes: int64(785)\n",
      "memory usage: 359.3 MB\n"
     ]
    }
   ],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "6e69e8e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_X shape: (60000, 784), train_Y shape: (60000, 10)\n",
      "test_X shape: (10000, 784), test_Y shape: (10000, 10)\n",
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.01176471 0.07058824 0.07058824 0.07058824\n",
      " 0.49411765 0.53333333 0.68627451 0.10196078 0.65098039 1.\n",
      " 0.96862745 0.49803922 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.11764706 0.14117647 0.36862745 0.60392157\n",
      " 0.66666667 0.99215686 0.99215686 0.99215686 0.99215686 0.99215686\n",
      " 0.88235294 0.6745098  0.99215686 0.94901961 0.76470588 0.25098039\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.19215686\n",
      " 0.93333333 0.99215686 0.99215686 0.99215686 0.99215686 0.99215686\n",
      " 0.99215686 0.99215686 0.99215686 0.98431373 0.36470588 0.32156863\n",
      " 0.32156863 0.21960784 0.15294118 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.07058824 0.85882353 0.99215686\n",
      " 0.99215686 0.99215686 0.99215686 0.99215686 0.77647059 0.71372549\n",
      " 0.96862745 0.94509804 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.31372549 0.61176471 0.41960784 0.99215686\n",
      " 0.99215686 0.80392157 0.04313725 0.         0.16862745 0.60392157\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.05490196 0.00392157 0.60392157 0.99215686 0.35294118\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.54509804 0.99215686 0.74509804 0.00784314 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.04313725\n",
      " 0.74509804 0.99215686 0.2745098  0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.1372549  0.94509804\n",
      " 0.88235294 0.62745098 0.42352941 0.00392157 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.31764706 0.94117647 0.99215686\n",
      " 0.99215686 0.46666667 0.09803922 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.17647059 0.72941176 0.99215686 0.99215686\n",
      " 0.58823529 0.10588235 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.0627451  0.36470588 0.98823529 0.99215686 0.73333333\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.97647059 0.99215686 0.97647059 0.25098039 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.18039216 0.50980392 0.71764706 0.99215686\n",
      " 0.99215686 0.81176471 0.00784314 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.15294118 0.58039216\n",
      " 0.89803922 0.99215686 0.99215686 0.99215686 0.98039216 0.71372549\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.09411765 0.44705882 0.86666667 0.99215686 0.99215686 0.99215686\n",
      " 0.99215686 0.78823529 0.30588235 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.09019608 0.25882353 0.83529412 0.99215686\n",
      " 0.99215686 0.99215686 0.99215686 0.77647059 0.31764706 0.00784314\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.07058824 0.67058824\n",
      " 0.85882353 0.99215686 0.99215686 0.99215686 0.99215686 0.76470588\n",
      " 0.31372549 0.03529412 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.21568627 0.6745098  0.88627451 0.99215686 0.99215686 0.99215686\n",
      " 0.99215686 0.95686275 0.52156863 0.04313725 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.53333333 0.99215686\n",
      " 0.99215686 0.99215686 0.83137255 0.52941176 0.51764706 0.0627451\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [1 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "# push the pixel value ranging from 0-255 to 0-1\n",
    "def normalize_df(df):\n",
    "    return df / 255.0\n",
    "\n",
    "\n",
    "train_df_np = train_df.to_numpy()\n",
    "test_df_np = test_df.to_numpy()\n",
    "\n",
    "train_X_original = train_df_np[:, 1:]  # all rows, columns from index 1 to end\n",
    "train_Y_original = train_df_np[:, 0]   # all rows, column at index 0\n",
    "test_X_original = test_df_np[:, 1:]  # all rows, columns from index 1 to end\n",
    "test_Y_original = test_df_np[:, 0]   # all rows, column at index 0\n",
    "\n",
    "train_X = normalize_df(train_X_original)  # normalize pixel values to range 0-1\n",
    "test_X = normalize_df(test_X_original)\n",
    "\n",
    "train_Y = np.array([np.array([1 if i == label else 0 for i in range(10)]) for label in train_Y_original])  # one-hot encode labels\n",
    "test_Y = np.array([np.array([1 if i == label else 0 for i in range(10)]) for label in test_Y_original])\n",
    "\n",
    "print(f\"train_X shape: {train_X.shape}, train_Y shape: {train_Y.shape}\")\n",
    "print(f\"test_X shape: {test_X.shape}, test_Y shape: {test_Y.shape}\")\n",
    "\n",
    "print(train_X[0])  # print first image pixel values\n",
    "print(train_Y)  # print first image label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26daf8a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "784\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.3\n",
    "epochs = 1000\n",
    "batch_size = 1024\n",
    "learning_rate_decay = 0.99\n",
    "\n",
    "print(train_X.shape[1])  # number of features (pixels) in each image\n",
    "neuron_layers = [train_X.shape[1], 64, 32, 10]  # 784 input neurons, 64 hidden neurons, 32 hidden neurons, 10 output neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "8b6dda66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights: [array([[-0.6075477 , -0.12613641, -0.68460636, ..., -0.0844797 ,\n",
      "         1.29201502, -0.17671057],\n",
      "       [ 1.68778715, -1.04661354,  0.64212021, ...,  0.30399334,\n",
      "         0.57309681, -1.21858776],\n",
      "       [-0.7714709 , -0.3525092 ,  3.43744895, ..., -1.12614983,\n",
      "         1.76101748,  1.00055791],\n",
      "       ...,\n",
      "       [ 0.15042069, -0.03143825, -0.47706241, ..., -0.20128847,\n",
      "         0.76979841, -0.79327421],\n",
      "       [ 1.074283  ,  0.91101506,  1.51821052, ...,  0.82372081,\n",
      "        -0.03305384, -0.72406442],\n",
      "       [-0.49549346,  0.10234107, -1.24625174, ..., -0.45337455,\n",
      "        -0.83266176, -0.14844853]], shape=(784, 64)), array([[ 1.16883084,  0.7803159 ,  2.9419728 , ..., -0.12693195,\n",
      "        -0.8939732 ,  1.5353488 ],\n",
      "       [ 1.24264102, -1.12174584,  0.56940394, ...,  0.36536019,\n",
      "        -2.17169936, -0.16151856],\n",
      "       [ 0.24521095,  1.23647198,  1.36720026, ...,  0.61615067,\n",
      "         1.20298308, -1.31427184],\n",
      "       ...,\n",
      "       [ 0.70095989,  0.66223801, -0.19567812, ...,  1.44806295,\n",
      "        -0.87384755, -0.19223417],\n",
      "       [-0.31425919, -0.95910968,  1.0992209 , ...,  0.77300973,\n",
      "        -3.09297923,  0.73341499],\n",
      "       [-0.53749377,  2.25261964,  1.65693446, ...,  1.12943212,\n",
      "        -0.91900785, -0.32499178]], shape=(64, 32)), array([[-7.61116549e-02, -3.73105408e-01, -4.33389747e-01,\n",
      "         4.52850640e-01,  2.32839497e-01, -1.54243498e+00,\n",
      "        -4.85113864e-01, -4.80365959e-01, -5.98704647e-01,\n",
      "         6.20223495e-01],\n",
      "       [-5.25243754e-01,  1.73178416e-01, -4.94252649e-01,\n",
      "        -9.39638012e-02,  1.13925644e+00,  1.03760592e+00,\n",
      "        -3.63399647e-01, -8.61447191e-01,  1.35535665e+00,\n",
      "        -9.94388395e-01],\n",
      "       [ 5.66338847e-01, -4.51119211e-01,  1.28413888e+00,\n",
      "        -2.23753203e+00, -1.00423957e+00, -4.84244996e-01,\n",
      "         1.22603283e+00,  3.48640392e-01,  8.74868932e-01,\n",
      "        -8.62649459e-01],\n",
      "       [ 3.08438972e-01,  1.85916845e+00, -9.76055273e-01,\n",
      "        -7.97174607e-01,  2.48683324e-01,  2.69609301e-01,\n",
      "        -1.04567819e+00,  6.66597958e-01,  1.09914338e-01,\n",
      "         6.56025300e-02],\n",
      "       [ 1.78518922e+00, -5.53355571e-01, -1.74983849e-01,\n",
      "         9.54251122e-01, -1.27650763e-01,  1.12527989e+00,\n",
      "        -5.04604633e-01,  6.38114922e-02,  1.00046519e+00,\n",
      "        -1.09777352e+00],\n",
      "       [-9.21255503e-01, -1.55749420e-01,  6.52706787e-01,\n",
      "        -1.15458399e+00, -6.80770565e-02,  1.12077994e+00,\n",
      "         6.22397996e-01, -1.59063466e+00, -1.79765936e+00,\n",
      "         1.48703127e-01],\n",
      "       [ 5.48919436e-01,  2.15248390e+00, -3.49209565e-01,\n",
      "        -1.14402254e+00, -3.18701415e-01, -6.97277236e-01,\n",
      "        -2.14637342e+00, -1.54763546e-01, -1.35309077e+00,\n",
      "        -6.84194983e-01],\n",
      "       [ 5.67111561e-01, -6.82599353e-01, -1.26265988e-01,\n",
      "         8.91497064e-01,  8.71428173e-01,  1.24685194e+00,\n",
      "        -9.44609470e-01, -2.25592990e+00,  1.59274037e+00,\n",
      "         5.93108109e-01],\n",
      "       [-1.64287483e+00,  1.30804757e+00,  1.22461519e+00,\n",
      "         4.89042665e-03, -3.18547416e-01, -3.02127166e-01,\n",
      "         1.00622283e+00,  9.88518546e-01,  5.55262382e-01,\n",
      "        -9.62574114e-01],\n",
      "       [-3.99393533e-01, -1.73515418e+00, -4.70369414e-01,\n",
      "         1.35397218e+00,  1.33520215e+00,  1.57137982e+00,\n",
      "         1.59335901e-01, -1.93996937e-01, -1.02985284e-01,\n",
      "         2.28702794e+00],\n",
      "       [ 6.66909617e-01,  1.49885146e-01,  2.55741505e-01,\n",
      "        -2.01054878e+00, -1.08074079e+00,  9.14796305e-02,\n",
      "         2.94793196e-01,  1.65453869e+00, -2.44423500e-01,\n",
      "         3.00864420e-01],\n",
      "       [-2.88703036e-01,  2.14419744e+00, -1.30549684e+00,\n",
      "        -4.71507500e-01, -5.24486960e-01, -6.22849871e-01,\n",
      "         1.98242202e+00,  1.06037867e+00, -1.10062945e+00,\n",
      "         9.24355521e-01],\n",
      "       [ 2.75472450e-01,  5.30413450e-02, -3.35080525e-01,\n",
      "        -8.92898880e-02,  1.43001032e+00,  1.43942104e+00,\n",
      "         5.64683252e-01,  1.02021169e+00, -1.76786005e+00,\n",
      "        -1.02330095e+00],\n",
      "       [ 6.54661038e-01,  1.48991593e+00,  3.14704485e-01,\n",
      "         9.78419608e-02, -1.02610571e+00,  2.70992691e+00,\n",
      "         4.62863770e-01, -5.39333220e-02, -8.26441959e-01,\n",
      "        -4.71111363e-02],\n",
      "       [-5.81489533e-01, -1.83100645e+00,  2.77583084e-01,\n",
      "         4.20603706e-01,  2.42237248e+00,  2.04177254e-01,\n",
      "        -6.10504412e-01, -4.40533112e-02,  5.92357377e-01,\n",
      "        -2.40626561e+00],\n",
      "       [-2.77963915e+00, -1.82001861e+00,  5.92303183e-02,\n",
      "         1.47035952e+00,  7.11697767e-01, -1.47375949e-01,\n",
      "        -3.93170704e-01,  1.46303351e+00,  3.22879637e-01,\n",
      "         1.54480179e-01],\n",
      "       [ 6.88276826e-01, -1.70600134e-01,  1.38801891e-02,\n",
      "         2.01997750e-02,  3.37695585e-01,  3.11618875e-01,\n",
      "         4.21671852e-01, -8.06431073e-01, -1.00818144e+00,\n",
      "         6.66457706e-01],\n",
      "       [-1.10912272e+00, -8.16748863e-01,  1.16443077e+00,\n",
      "        -6.41407149e-01, -2.51551735e-03, -5.47217305e-02,\n",
      "        -4.14728659e-01, -2.40720668e+00, -1.41184933e+00,\n",
      "         9.67635609e-01],\n",
      "       [ 2.97002083e-01, -8.01608676e-01,  2.19168391e-01,\n",
      "        -6.18712066e-02, -4.55088954e-01,  1.51730618e+00,\n",
      "        -6.34956715e-01,  8.75543652e-01,  1.53711970e-01,\n",
      "         1.59306370e+00],\n",
      "       [-6.97676322e-02,  3.11617465e-01,  6.88505821e-01,\n",
      "        -4.17068823e-01,  2.11923095e+00,  1.52550991e+00,\n",
      "         1.86851750e+00, -4.21694346e-01, -8.48133475e-01,\n",
      "         6.39189393e-01],\n",
      "       [ 1.84798054e+00, -2.13503803e-01,  3.12577135e-02,\n",
      "         2.32515928e+00,  3.13882079e-01, -1.62750844e-01,\n",
      "        -9.63505084e-01, -1.03762053e+00,  1.26827978e+00,\n",
      "         1.31502881e-01],\n",
      "       [ 1.88099842e+00, -1.22091675e+00, -1.33748364e+00,\n",
      "        -2.01520614e-01, -1.71371235e-01, -3.70993385e-01,\n",
      "        -1.19262257e+00,  1.80140696e+00,  2.82945082e-01,\n",
      "        -1.57327911e-01],\n",
      "       [ 8.58293641e-01, -1.31876990e+00,  1.25370407e+00,\n",
      "         7.55487260e-01,  1.45471203e+00, -2.84261709e-02,\n",
      "        -9.52924042e-01,  1.38894388e+00,  8.51335711e-01,\n",
      "        -8.00120158e-02],\n",
      "       [-1.60806165e+00,  3.84169721e-01,  3.26073080e-01,\n",
      "        -1.84028569e+00,  7.61452906e-01,  3.85076742e-01,\n",
      "        -3.94135962e-01,  6.43228054e-01,  1.62372197e-01,\n",
      "         3.10666718e-01],\n",
      "       [ 4.55053316e-01, -8.25704635e-01,  5.67157645e-01,\n",
      "        -1.80047517e+00, -2.36573434e-01,  5.06525919e-02,\n",
      "        -1.33732044e-01,  1.12891414e+00, -4.96022443e-01,\n",
      "        -1.26020745e+00],\n",
      "       [-5.73129525e-01, -9.03284092e-01, -9.33235963e-01,\n",
      "         3.00920281e-01, -4.52325972e-01, -8.24996807e-01,\n",
      "        -6.56319040e-01,  1.32049538e+00, -7.41292975e-01,\n",
      "        -4.82785330e-01],\n",
      "       [-1.96167726e+00,  3.12039633e-01, -9.05138587e-01,\n",
      "         2.04399164e-01, -2.80073690e-01, -1.02617673e+00,\n",
      "         2.21280931e-01,  1.10499974e+00, -7.81753478e-01,\n",
      "        -3.52068287e-01],\n",
      "       [-5.51607498e-01, -1.58835977e+00, -3.38090665e+00,\n",
      "         2.30620109e-01,  1.35335026e+00, -9.03523341e-01,\n",
      "        -1.05469216e-01,  1.37245524e-01,  8.96243039e-01,\n",
      "         1.39461343e+00],\n",
      "       [-2.97311719e-01, -9.65037364e-01, -1.85420494e+00,\n",
      "        -8.49608358e-01, -8.62925406e-01, -1.41873093e-01,\n",
      "        -7.35750048e-01, -1.54658432e+00, -1.32872436e+00,\n",
      "         8.76130561e-01],\n",
      "       [ 3.41750477e-01,  5.54988366e-01, -6.15825491e-01,\n",
      "        -1.50730759e+00, -7.46584270e-02,  2.00466006e+00,\n",
      "         1.86008618e-01, -8.14951378e-01,  1.74889878e-01,\n",
      "        -7.99420302e-01],\n",
      "       [-6.38950161e-01,  1.65190811e+00,  1.99170722e-01,\n",
      "         4.69944542e-01, -6.33492442e-01,  6.26520347e-01,\n",
      "        -1.27225722e+00,  7.82940750e-01, -6.64008502e-01,\n",
      "         1.35587200e+00],\n",
      "       [ 4.41007308e-01, -4.99099370e-01,  5.12648639e-01,\n",
      "        -9.33338035e-01, -7.77258609e-01,  9.16786533e-02,\n",
      "        -2.79867831e-01, -1.93024196e+00, -1.49938278e+00,\n",
      "        -2.51690882e-01]])]\n",
      "Biases: [array([-0.33503558,  0.56198764, -0.03235575,  0.2953818 , -0.21016473,\n",
      "       -0.26879262, -0.02036115, -0.54616803,  1.83146099, -0.97634551,\n",
      "       -1.27438618, -0.0493496 , -1.24426041, -3.0036448 , -0.25141871,\n",
      "       -0.1619704 , -0.82029897,  0.90442672,  0.12155252,  0.02524571,\n",
      "        0.20607651, -1.8836019 , -1.28642409, -1.05579767,  0.57689602,\n",
      "        1.68331248,  1.14057664, -0.02610063,  0.57867547, -0.11990455,\n",
      "        0.39700316,  0.40182457,  0.50204569,  0.51146551,  0.87693178,\n",
      "        0.36058245,  0.31645327,  2.15614112, -1.4942945 , -1.21303078,\n",
      "        0.87079988, -0.33908356, -0.98532488,  0.27890114, -2.95615317,\n",
      "       -0.48197537, -0.56675705, -1.37286109,  0.26901427,  0.05566027,\n",
      "        0.90650833, -0.62782798, -0.7139183 , -1.27172516, -0.50752455,\n",
      "       -0.32001113, -1.12564602, -0.6299495 , -1.13879394,  0.22564506,\n",
      "       -0.17279668, -0.02808452, -0.512401  , -1.96392608]), array([ 0.9884023 ,  0.49431415, -0.63816553,  0.98938162,  0.85583169,\n",
      "        0.82202522, -2.6492765 ,  2.43573379, -0.94878722,  0.35127286,\n",
      "       -0.18411288, -0.30503935,  1.19282639, -0.81251768,  1.62396234,\n",
      "       -0.35221287, -1.68218306,  0.15687482, -0.61010395,  1.56683906,\n",
      "       -0.58714038, -0.13799262, -0.71732206,  0.46376419,  0.78773593,\n",
      "        0.61712284,  0.61532828, -1.05676957,  0.75777   , -0.71103252,\n",
      "       -0.37279494, -0.38435293]), array([ 0.67157433, -1.3600312 ,  0.46844275, -1.21094513,  2.15313095,\n",
      "        1.02582159, -1.14677845,  0.49711051, -0.25732866, -1.52221973])]\n"
     ]
    }
   ],
   "source": [
    "# Initialize weights and biases for each layer\n",
    "np.random.seed(40)  # for reproducibility\n",
    "\n",
    "w = []\n",
    "for i in range(len(neuron_layers) - 1):\n",
    "    # Matrix of weights for Layer i to Layer i+1\n",
    "    weights = np.random.randn(neuron_layers[i], neuron_layers[i + 1]) # matrix of neuronal_layers[i] rows and neuronal_layers[i+1] columns\n",
    "    w.append(weights)\n",
    "\n",
    "b = []\n",
    "for i in range(1, len(neuron_layers)):\n",
    "    # Bias vector for Layer i\n",
    "    biases = np.random.randn(neuron_layers[i]) # vector of neuronal_layers[i] entries\n",
    "    b.append(biases)\n",
    "\n",
    "print(\"Weights:\", w)\n",
    "print(\"Biases:\", b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "4b39b8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "4f94b7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feed_forward_neural_network(neuron_vals, w, b):\n",
    "    activations = [neuron_vals]\n",
    "\n",
    "    for layer_weights, layer_biases in zip(w, b):\n",
    "        neuron_vals_before_activation = neuron_vals @ layer_weights + layer_biases # first matrix multiplication, then + bias\n",
    "\n",
    "        if layer_weights is not w[-1]:\n",
    "            # we only want to apply the activation function to the hidden layers, not the output layer\n",
    "            neuron_vals = sigmoid(neuron_vals_before_activation)\n",
    "        else:\n",
    "            neuron_vals = neuron_vals_before_activation  # output layer without activation function\n",
    "\n",
    "        ######## ------------- WHY NO ACTIVATION FUNCTION IN THE OUTPUT LAYER? ------------- ########\n",
    "        # !!! here was a mistake before, I applied the sigmoid function to the output layer as well\n",
    "        # for multi-class predictions we don't want to do that tho, because then the softmax function produces unsatisfying results\n",
    "        # the values after the softmax function are not as \"confident\" and are pretty flat\n",
    "        # the softmax function can also handle negative values, which we would not have if we applied the sigmoid function before\n",
    "        # e.g. if the values are really small after the sigmoid (close to 0), the softmax would go close to (1/number_of_classes) for the wrong classes and also flattening the correct class, which is not what we want\n",
    "\n",
    "        activations.append(neuron_vals)\n",
    "        \n",
    "    return activations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "5e16b96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    output = np.exp(z) / np.sum(np.exp(z), axis=1, keepdims=True) # axis=1 means sum each row, keepdims=True ensure the result is (n, 1) and not (n,)\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "a2fbd12b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 9.024204061681658\n"
     ]
    }
   ],
   "source": [
    "def calc_log_loss(features, labels, w, b):\n",
    "    pred_label = feed_forward_neural_network(features, w, b)[-1]\n",
    "    pred_label = softmax(pred_label)\n",
    "\n",
    "    pred_label = np.clip(pred_label, 1e-10, 1 - 1e-10)\n",
    "    log_loss = np.mean(np.sum(-labels * np.log(pred_label), axis=1))\n",
    "    # using categorical cross-entropy loss function, because we have multiple classes (10 digits)\n",
    "    # otherwise, when we would use binary cross-entropy loss function, the problem would push other values more and more to zero => softmax function already does that\n",
    "    # (probabilities for other classes automatically go lower when the correct class probability goes higher)\n",
    "    return log_loss\n",
    "\n",
    "log_loss = calc_log_loss(train_X, train_Y, w, b)\n",
    "print(f\"Loss: {log_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "4608e9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_gradients(features, labels, w, b):\n",
    "    dB = [np.zeros_like(bias) for bias in b]  # gradient for biases\n",
    "    dW = [np.zeros_like(weights) for weights in w]  # gradient for weights\n",
    "\n",
    "    activations = feed_forward_neural_network(features, w, b)\n",
    "    predictions = softmax(activations[-1])  # apply softmax to the output layer activations\n",
    "    \n",
    "    delta = predictions - labels    # initial delta (dL/da)\n",
    "    ####### ------------- WHY CAN WE JUST WRITE THE dL/dz FUNCTION LIKE THIS? ------------- #######\n",
    "    # when using softmax + categorical cross-entropy loss function, the derivative of the loss with respect to the z-function (dL/dz) is simply (predictions - labels)\n",
    "    # they cancel each other out, simplifying like this\n",
    "\n",
    "    for idx in range(len(w)-1, -1, -1):\n",
    "        if (idx == len(w)-1):\n",
    "            pass\n",
    "        else:\n",
    "            layer_activations = activations[idx+1]    # current layer activations to calculate the sigmoid derivative\n",
    "            der_sigmoid = layer_activations * (1 - layer_activations)   # derivative of sigmoid\n",
    "\n",
    "\n",
    "            following_weights = w[idx + 1]  # weights of the next layer to calculate the derivative of the z-function (dz/da) with respect to the activations\n",
    "\n",
    "            delta = delta @ following_weights.T * der_sigmoid  # update delta for next layer (dz/da * da/dz)\n",
    "    \n",
    "        der_weights = activations[idx].T  # previous layer activations to calculate the z-derivative (dz/dw)\n",
    "\n",
    "        dW[idx] = der_weights @ delta / features.shape[0]  # gradient for weights (dz/dw)  => shape[0] is the number of rows (num samples)\n",
    "        # we have multiple der_weights arrays (one for each previous sample), so for every previous neuron we multiply take the mean from all samples for every of the previous neurons\n",
    "        # dW has the amount of previous neurons rows and the amount of current neurons columns\n",
    "        # (first row = first previous neuron, first column = first current neuron, second column = second current neuron, ...)\n",
    "        \n",
    "        dB[idx] = delta.mean(axis=0) # gradient for bias (dz/db)\n",
    "    return dW, dB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "f46e5e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_weights_and_biases(w, b, dW, dB, learning_rate, epoch):\n",
    "    #print(\"Weights before update:\", w)\n",
    "    #print(\"Weight Gradients:\", dW)\n",
    "\n",
    "    for i in range(len(w)):\n",
    "        w[i] -= learning_rate * dW[i] * learning_rate_decay ** (epoch * 0.1)\n",
    "        b[i] -= learning_rate * dB[i] * learning_rate_decay ** (epoch * 0.1)\n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "04e85315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Loss: 9.024204061681658\n",
      "Epoch 0, Training Loss: 1.7152873711834622; Validation Loss: 1.7107858946692687\n",
      "Epoch 100, Training Loss: 0.23903251624356042; Validation Loss: 0.28098734192587643\n",
      "Epoch 200, Training Loss: 0.17154918315035056; Validation Loss: 0.23295071346901677\n",
      "Epoch 300, Training Loss: 0.13820296306601376; Validation Loss: 0.2137825857795433\n",
      "Epoch 400, Training Loss: 0.11724797854357714; Validation Loss: 0.20452366281066336\n",
      "Epoch 500, Training Loss: 0.10265368600918927; Validation Loss: 0.19998740442643176\n",
      "Epoch 600, Training Loss: 0.09168869690647191; Validation Loss: 0.19737077789329063\n",
      "Epoch 700, Training Loss: 0.08326482086259605; Validation Loss: 0.19719672462723406\n",
      "Epoch 800, Training Loss: 0.07654686338901902; Validation Loss: 0.19691978573053412\n",
      "Epoch 900, Training Loss: 0.07109351023801833; Validation Loss: 0.19776527241299025\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[304]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m     18\u001b[39m             loss = calc_log_loss(train_X, train_Y, w, b)\n\u001b[32m     19\u001b[39m             \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Training Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m; Validation Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcalc_log_loss(test_X,\u001b[38;5;250m \u001b[39mtest_Y,\u001b[38;5;250m \u001b[39mw,\u001b[38;5;250m \u001b[39mb)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m \u001b[43mtrain_network\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_X\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_Y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[304]\u001b[39m\u001b[32m, line 14\u001b[39m, in \u001b[36mtrain_network\u001b[39m\u001b[34m(train_X, train_Y, w, b)\u001b[39m\n\u001b[32m     11\u001b[39m     batch_X = train_X[batch_indices]\n\u001b[32m     12\u001b[39m     batch_Y = train_Y[batch_indices]\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m     dW, dB = \u001b[43mcalc_gradients\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_X\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_Y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m     w, b = update_weights_and_biases(w, b, dW, dB, learning_rate, epoch)\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m epoch % \u001b[32m100\u001b[39m == \u001b[32m0\u001b[39m:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[302]\u001b[39m\u001b[32m, line 1\u001b[39m, in \u001b[36mcalc_gradients\u001b[39m\u001b[34m(features, labels, w, b)\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcalc_gradients\u001b[39m(features, labels, w, b):\n\u001b[32m      2\u001b[39m     dB = [np.zeros_like(bias) \u001b[38;5;28;01mfor\u001b[39;00m bias \u001b[38;5;129;01min\u001b[39;00m b]  \u001b[38;5;66;03m# gradient for biases\u001b[39;00m\n\u001b[32m      3\u001b[39m     dW = [np.zeros_like(weights) \u001b[38;5;28;01mfor\u001b[39;00m weights \u001b[38;5;129;01min\u001b[39;00m w]  \u001b[38;5;66;03m# gradient for weights\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "def train_network(train_X, train_Y, w, b):\n",
    "    initial_loss = calc_log_loss(train_X, train_Y, w, b)\n",
    "    print(\"Initial Loss:\", initial_loss)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        indices = np.random.permutation(train_X.shape[0])\n",
    "\n",
    "        for start_idx in range(0, train_X.shape[0], batch_size):\n",
    "            end_idx = start_idx + batch_size\n",
    "            batch_indices = indices[start_idx:end_idx]\n",
    "            batch_X = train_X[batch_indices]\n",
    "            batch_Y = train_Y[batch_indices]\n",
    "\n",
    "            dW, dB = calc_gradients(batch_X, batch_Y, w, b)\n",
    "            w, b = update_weights_and_biases(w, b, dW, dB, learning_rate, epoch)\n",
    "\n",
    "        if epoch % 100 == 0:\n",
    "            loss = calc_log_loss(train_X, train_Y, w, b)\n",
    "            print(f\"Epoch {epoch}, Training Loss: {loss}; Validation Loss: {calc_log_loss(test_X, test_Y, w, b)}\")\n",
    "\n",
    "\n",
    "train_network(train_X, train_Y, w, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "44f15633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.94448975e-06 2.64049542e-08 1.74639533e-06 ... 9.99794931e-01\n",
      "  9.74004156e-07 1.61264059e-04]\n",
      " [5.26848015e-04 2.50990725e-03 9.94494334e-01 ... 1.94564627e-05\n",
      "  8.60606362e-05 3.54967161e-07]\n",
      " [8.32764365e-08 9.98467070e-01 5.61167001e-04 ... 6.94096659e-04\n",
      "  1.79720817e-04 1.50280872e-07]\n",
      " ...\n",
      " [1.51767316e-07 2.39612091e-07 3.07409665e-08 ... 2.71578214e-05\n",
      "  6.33771849e-05 4.83392322e-05]\n",
      " [4.73753164e-09 4.56051638e-03 1.69109058e-06 ... 8.41496468e-07\n",
      "  1.23915856e-05 1.67385897e-07]\n",
      " [3.59174855e-06 4.18511328e-07 1.81056694e-05 ... 4.92296125e-09\n",
      "  2.14283610e-07 1.07511895e-09]]\n",
      "Accuracy: 94.57%\n"
     ]
    }
   ],
   "source": [
    "def predict(test_X, test_Y, w, b):\n",
    "    activations = feed_forward_neural_network(test_X, w, b)\n",
    "    predictions = activations[-1]\n",
    "    predictions = softmax(predictions)\n",
    "    print(predictions)\n",
    "\n",
    "    predicted_classes = np.argmax(predictions, axis=1)\n",
    "    true_classes = np.argmax(test_Y, axis=1)\n",
    "\n",
    "    accuracy = np.mean(predicted_classes == true_classes)\n",
    "    print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "predict(test_X, test_Y, w, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14897ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save weights and biases\n",
    "weight_dict = {f'weights_{i}': w[i] for i in range(len(w))}\n",
    "bias_dict = {f'biases_{i}': b[i] for i in range(len(b))}\n",
    "\n",
    "file_name = \"models/mnist_model_v5_xx_xx_batched.npz\"\n",
    "\n",
    "np.savez(file_name, **weight_dict, **bias_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1c0fc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: 9\n"
     ]
    }
   ],
   "source": [
    "# load weights and biases from file\n",
    "file_name = \"models/mnist_model_v3_94_30_batched.npz\"\n",
    "data = np.load(file_name)\n",
    "w = [data[f'weights_{i}'] for i in range(len(w))]\n",
    "b = [data[f'biases_{i}'] for i in range(len(b))]\n",
    "\n",
    "# plot a picture of a number\n",
    "def plot_number(index):\n",
    "    image = test_X[index].reshape(28, 28)  # reshape the flat array to a 28x28 matrix\n",
    "    plt.imshow(image, cmap='gray')\n",
    "    plt.title(f\"Label: {test_Y_original[index]}\")\n",
    "    plt.axis('off')  # turn off axis labels\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def get_number_prediction(index):\n",
    "    image = test_X[index:index+1]\n",
    "    activations = feed_forward_neural_network(image, w, b)\n",
    "    predictions = softmax(activations[-1])\n",
    "\n",
    "    predicted_class = np.argmax(predictions, axis=1)[0]\n",
    "    print(f\"Predicted class: {predicted_class}\")\n",
    "\n",
    "    return predicted_class\n",
    "\n",
    "def predict_index(index=0):\n",
    "    plot_number(index)\n",
    "    get_number_prediction(index)\n",
    "\n",
    "predict_index(12)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuronales-netz",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
