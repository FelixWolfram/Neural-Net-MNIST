{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6171c630",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_df = pd.read_csv(\"data/mnist_train.csv\")\n",
    "test_df = pd.read_csv(\"data/mnist_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "244bcc21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>1x1</th>\n",
       "      <th>1x2</th>\n",
       "      <th>1x3</th>\n",
       "      <th>1x4</th>\n",
       "      <th>1x5</th>\n",
       "      <th>1x6</th>\n",
       "      <th>1x7</th>\n",
       "      <th>1x8</th>\n",
       "      <th>1x9</th>\n",
       "      <th>...</th>\n",
       "      <th>28x19</th>\n",
       "      <th>28x20</th>\n",
       "      <th>28x21</th>\n",
       "      <th>28x22</th>\n",
       "      <th>28x23</th>\n",
       "      <th>28x24</th>\n",
       "      <th>28x25</th>\n",
       "      <th>28x26</th>\n",
       "      <th>28x27</th>\n",
       "      <th>28x28</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  1x1  1x2  1x3  1x4  1x5  1x6  1x7  1x8  1x9  ...  28x19  28x20  \\\n",
       "0      5    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "1      0    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "2      4    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "3      1    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "4      9    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "\n",
       "   28x21  28x22  28x23  28x24  28x25  28x26  28x27  28x28  \n",
       "0      0      0      0      0      0      0      0      0  \n",
       "1      0      0      0      0      0      0      0      0  \n",
       "2      0      0      0      0      0      0      0      0  \n",
       "3      0      0      0      0      0      0      0      0  \n",
       "4      0      0      0      0      0      0      0      0  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44ec129a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>1x1</th>\n",
       "      <th>1x2</th>\n",
       "      <th>1x3</th>\n",
       "      <th>1x4</th>\n",
       "      <th>1x5</th>\n",
       "      <th>1x6</th>\n",
       "      <th>1x7</th>\n",
       "      <th>1x8</th>\n",
       "      <th>1x9</th>\n",
       "      <th>...</th>\n",
       "      <th>28x19</th>\n",
       "      <th>28x20</th>\n",
       "      <th>28x21</th>\n",
       "      <th>28x22</th>\n",
       "      <th>28x23</th>\n",
       "      <th>28x24</th>\n",
       "      <th>28x25</th>\n",
       "      <th>28x26</th>\n",
       "      <th>28x27</th>\n",
       "      <th>28x28</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>60000.000000</td>\n",
       "      <td>60000.0</td>\n",
       "      <td>60000.0</td>\n",
       "      <td>60000.0</td>\n",
       "      <td>60000.0</td>\n",
       "      <td>60000.0</td>\n",
       "      <td>60000.0</td>\n",
       "      <td>60000.0</td>\n",
       "      <td>60000.0</td>\n",
       "      <td>60000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>60000.000000</td>\n",
       "      <td>60000.000000</td>\n",
       "      <td>60000.000000</td>\n",
       "      <td>60000.000000</td>\n",
       "      <td>60000.000000</td>\n",
       "      <td>60000.0000</td>\n",
       "      <td>60000.0</td>\n",
       "      <td>60000.0</td>\n",
       "      <td>60000.0</td>\n",
       "      <td>60000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4.453933</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.200433</td>\n",
       "      <td>0.088867</td>\n",
       "      <td>0.045633</td>\n",
       "      <td>0.019283</td>\n",
       "      <td>0.015117</td>\n",
       "      <td>0.0020</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.889270</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>6.042472</td>\n",
       "      <td>3.956189</td>\n",
       "      <td>2.839845</td>\n",
       "      <td>1.686770</td>\n",
       "      <td>1.678283</td>\n",
       "      <td>0.3466</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>9.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>254.000000</td>\n",
       "      <td>254.000000</td>\n",
       "      <td>253.000000</td>\n",
       "      <td>253.000000</td>\n",
       "      <td>254.000000</td>\n",
       "      <td>62.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              label      1x1      1x2      1x3      1x4      1x5      1x6  \\\n",
       "count  60000.000000  60000.0  60000.0  60000.0  60000.0  60000.0  60000.0   \n",
       "mean       4.453933      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "std        2.889270      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "min        0.000000      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "25%        2.000000      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "50%        4.000000      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "75%        7.000000      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "max        9.000000      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "\n",
       "           1x7      1x8      1x9  ...         28x19         28x20  \\\n",
       "count  60000.0  60000.0  60000.0  ...  60000.000000  60000.000000   \n",
       "mean       0.0      0.0      0.0  ...      0.200433      0.088867   \n",
       "std        0.0      0.0      0.0  ...      6.042472      3.956189   \n",
       "min        0.0      0.0      0.0  ...      0.000000      0.000000   \n",
       "25%        0.0      0.0      0.0  ...      0.000000      0.000000   \n",
       "50%        0.0      0.0      0.0  ...      0.000000      0.000000   \n",
       "75%        0.0      0.0      0.0  ...      0.000000      0.000000   \n",
       "max        0.0      0.0      0.0  ...    254.000000    254.000000   \n",
       "\n",
       "              28x21         28x22         28x23       28x24    28x25    28x26  \\\n",
       "count  60000.000000  60000.000000  60000.000000  60000.0000  60000.0  60000.0   \n",
       "mean       0.045633      0.019283      0.015117      0.0020      0.0      0.0   \n",
       "std        2.839845      1.686770      1.678283      0.3466      0.0      0.0   \n",
       "min        0.000000      0.000000      0.000000      0.0000      0.0      0.0   \n",
       "25%        0.000000      0.000000      0.000000      0.0000      0.0      0.0   \n",
       "50%        0.000000      0.000000      0.000000      0.0000      0.0      0.0   \n",
       "75%        0.000000      0.000000      0.000000      0.0000      0.0      0.0   \n",
       "max      253.000000    253.000000    254.000000     62.0000      0.0      0.0   \n",
       "\n",
       "         28x27    28x28  \n",
       "count  60000.0  60000.0  \n",
       "mean       0.0      0.0  \n",
       "std        0.0      0.0  \n",
       "min        0.0      0.0  \n",
       "25%        0.0      0.0  \n",
       "50%        0.0      0.0  \n",
       "75%        0.0      0.0  \n",
       "max        0.0      0.0  \n",
       "\n",
       "[8 rows x 785 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0988868c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 60000 entries, 0 to 59999\n",
      "Columns: 785 entries, label to 28x28\n",
      "dtypes: int64(785)\n",
      "memory usage: 359.3 MB\n"
     ]
    }
   ],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e69e8e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_X shape: (60000, 784), train_Y shape: (60000, 10)\n",
      "test_X shape: (10000, 784), test_Y shape: (10000, 10)\n",
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.01176471 0.07058824 0.07058824 0.07058824\n",
      " 0.49411765 0.53333333 0.68627451 0.10196078 0.65098039 1.\n",
      " 0.96862745 0.49803922 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.11764706 0.14117647 0.36862745 0.60392157\n",
      " 0.66666667 0.99215686 0.99215686 0.99215686 0.99215686 0.99215686\n",
      " 0.88235294 0.6745098  0.99215686 0.94901961 0.76470588 0.25098039\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.19215686\n",
      " 0.93333333 0.99215686 0.99215686 0.99215686 0.99215686 0.99215686\n",
      " 0.99215686 0.99215686 0.99215686 0.98431373 0.36470588 0.32156863\n",
      " 0.32156863 0.21960784 0.15294118 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.07058824 0.85882353 0.99215686\n",
      " 0.99215686 0.99215686 0.99215686 0.99215686 0.77647059 0.71372549\n",
      " 0.96862745 0.94509804 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.31372549 0.61176471 0.41960784 0.99215686\n",
      " 0.99215686 0.80392157 0.04313725 0.         0.16862745 0.60392157\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.05490196 0.00392157 0.60392157 0.99215686 0.35294118\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.54509804 0.99215686 0.74509804 0.00784314 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.04313725\n",
      " 0.74509804 0.99215686 0.2745098  0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.1372549  0.94509804\n",
      " 0.88235294 0.62745098 0.42352941 0.00392157 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.31764706 0.94117647 0.99215686\n",
      " 0.99215686 0.46666667 0.09803922 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.17647059 0.72941176 0.99215686 0.99215686\n",
      " 0.58823529 0.10588235 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.0627451  0.36470588 0.98823529 0.99215686 0.73333333\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.97647059 0.99215686 0.97647059 0.25098039 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.18039216 0.50980392 0.71764706 0.99215686\n",
      " 0.99215686 0.81176471 0.00784314 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.15294118 0.58039216\n",
      " 0.89803922 0.99215686 0.99215686 0.99215686 0.98039216 0.71372549\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.09411765 0.44705882 0.86666667 0.99215686 0.99215686 0.99215686\n",
      " 0.99215686 0.78823529 0.30588235 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.09019608 0.25882353 0.83529412 0.99215686\n",
      " 0.99215686 0.99215686 0.99215686 0.77647059 0.31764706 0.00784314\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.07058824 0.67058824\n",
      " 0.85882353 0.99215686 0.99215686 0.99215686 0.99215686 0.76470588\n",
      " 0.31372549 0.03529412 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.21568627 0.6745098  0.88627451 0.99215686 0.99215686 0.99215686\n",
      " 0.99215686 0.95686275 0.52156863 0.04313725 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.53333333 0.99215686\n",
      " 0.99215686 0.99215686 0.83137255 0.52941176 0.51764706 0.0627451\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [1 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "# push the pixel value ranging from 0-255 to 0-1\n",
    "def normalize_df(df):\n",
    "    return df / 255.0\n",
    "\n",
    "\n",
    "train_df_np = train_df.to_numpy()\n",
    "test_df_np = test_df.to_numpy()\n",
    "\n",
    "train_X_original = train_df_np[:, 1:]  # all rows, columns from index 1 to end\n",
    "train_Y_original = train_df_np[:, 0]   # all rows, column at index 0\n",
    "test_X_original = test_df_np[:, 1:]  # all rows, columns from index 1 to end\n",
    "test_Y_original = test_df_np[:, 0]   # all rows, column at index 0\n",
    "\n",
    "train_X = normalize_df(train_X_original)  # normalize pixel values to range 0-1\n",
    "test_X = normalize_df(test_X_original)\n",
    "\n",
    "train_Y = np.array([np.array([1 if i == label else 0 for i in range(10)]) for label in train_Y_original])  # one-hot encode labels\n",
    "test_Y = np.array([np.array([1 if i == label else 0 for i in range(10)]) for label in test_Y_original])\n",
    "\n",
    "print(f\"train_X shape: {train_X.shape}, train_Y shape: {train_Y.shape}\")\n",
    "print(f\"test_X shape: {test_X.shape}, test_Y shape: {test_Y.shape}\")\n",
    "\n",
    "print(train_X[0])  # print first image pixel values\n",
    "print(train_Y)  # print first image label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17e2390",
   "metadata": {},
   "source": [
    "<h1>Start Training</h1>\n",
    "<p>---------------------------------------</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "26daf8a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "784\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1.5\n",
    "epochs = 5000\n",
    "\n",
    "print(train_X.shape[1])  # number of features (pixels) in each image\n",
    "neuron_layers = [train_X.shape[1], 24, 12, 10]  # 784 input neurons, 24 hidden neurons, 12 hidden neurons, 10 output neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8b6dda66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights: [array([[-0.6075477 , -0.12613641, -0.68460636, ...,  0.10895031,\n",
      "         0.03172679,  1.27263986],\n",
      "       [ 1.0714479 ,  0.41581801,  1.55067923, ...,  0.26671631,\n",
      "         0.00898941,  0.64110275],\n",
      "       [-0.17770716,  0.69627761, -1.1887251 , ..., -0.02070278,\n",
      "        -1.81352177,  0.20352189],\n",
      "       ...,\n",
      "       [-2.20997045, -1.48827842,  1.23193263, ...,  0.91332012,\n",
      "        -0.11148796,  0.10840572],\n",
      "       [-0.34346909, -0.22138858,  0.74352927, ..., -1.54694635,\n",
      "         0.08889181,  0.26012462],\n",
      "       [ 0.72573178, -1.15947005, -0.51378388, ..., -0.48199419,\n",
      "         1.44721549, -0.16274366]], shape=(784, 24)), array([[-3.69066815e-01,  5.89651330e-01, -1.43247965e-01,\n",
      "         7.89850644e-01,  3.57409046e-01, -9.60205803e-02,\n",
      "        -6.52531473e-01,  2.27379326e-01,  2.76057884e-01,\n",
      "         2.33166299e+00, -9.00951794e-01, -4.01972181e-01],\n",
      "       [-4.68800921e-02,  2.62033793e-01,  1.81470169e+00,\n",
      "        -8.35398361e-01,  1.21418174e+00, -2.24189626e+00,\n",
      "         7.17466948e-01, -1.63468532e-01,  3.24116249e+00,\n",
      "        -9.53643418e-01,  9.56584453e-02, -4.09994480e-03],\n",
      "       [-6.24905821e-01, -5.44281940e-02, -5.39119077e-01,\n",
      "        -2.35941988e-01, -3.78245330e-01,  3.13654214e-01,\n",
      "        -1.23110727e+00,  1.41275058e+00, -8.85442347e-01,\n",
      "        -1.11362834e+00, -7.62425234e-02, -1.67172664e+00],\n",
      "       [-5.25766837e-01,  1.07666836e+00, -6.74365642e-01,\n",
      "        -8.65380373e-01,  9.00981357e-01,  5.44600697e-01,\n",
      "        -2.13825955e+00, -5.64439614e-02, -4.81739032e-01,\n",
      "        -5.05881863e-03,  3.33871137e-01,  2.39787646e-01],\n",
      "       [ 6.25045068e-01,  1.26111913e+00,  4.44169279e-01,\n",
      "        -1.46541686e+00,  5.82966857e-01,  5.33459112e-01,\n",
      "        -1.80683091e+00,  9.38669495e-01,  1.09926594e+00,\n",
      "        -1.43176691e+00, -1.29808025e+00, -3.27513806e-01],\n",
      "       [-5.41999079e-01,  3.01882021e-02, -7.53687952e-01,\n",
      "        -3.73454061e-01,  7.86755403e-01,  1.27781279e+00,\n",
      "         1.48785205e+00,  5.07491906e-01,  1.54848294e+00,\n",
      "         1.82650965e+00,  2.95544529e-01, -7.92989610e-01],\n",
      "       [ 7.50497728e-01,  3.10932164e-01, -7.18942430e-01,\n",
      "         7.63115487e-01, -8.35754199e-01,  2.71592507e-01,\n",
      "         3.32837189e-02,  4.67109334e-01,  1.13990487e+00,\n",
      "         1.94114029e-01, -8.62356714e-01,  1.53846055e-01],\n",
      "       [-1.22737331e+00,  6.85125522e-01,  4.59005211e-01,\n",
      "         1.60159842e+00, -4.94149325e-02,  4.95459755e-02,\n",
      "         1.70455860e+00, -2.71911233e-01,  2.52205970e+00,\n",
      "        -1.57233441e-01,  7.67877390e-01,  1.27551055e+00],\n",
      "       [-2.75804420e+00, -4.25880493e-02,  9.01724649e-01,\n",
      "         5.92300526e-01, -3.02418719e-01, -1.36727718e+00,\n",
      "         1.60861943e+00, -7.57867373e-01,  1.56952612e+00,\n",
      "         6.15941453e-01, -4.31052284e-01, -9.99328584e-02],\n",
      "       [-9.06953767e-01, -8.59291543e-02, -1.03420351e+00,\n",
      "         2.01874627e-01,  2.34521794e-01,  2.11068381e-01,\n",
      "        -2.23202170e+00,  1.02310165e+00,  9.86400682e-01,\n",
      "        -2.19633198e-01, -5.32693959e-01, -1.47503484e+00],\n",
      "       [-4.22405626e-01,  2.37699119e+00, -1.08791460e+00,\n",
      "         9.38733599e-01,  2.57836333e+00,  3.03617782e-01,\n",
      "        -9.90574950e-01, -1.92566163e+00,  1.80383477e-01,\n",
      "         1.86323223e-03,  3.66854140e-02, -1.87902996e-01],\n",
      "       [-9.60142240e-01, -3.99199301e-01, -1.26201540e+00,\n",
      "         1.03147512e+00,  1.58198361e+00,  2.99012643e-01,\n",
      "        -1.22566187e+00,  6.47190864e-01,  5.93707083e-01,\n",
      "         1.37951713e+00,  4.95001820e-01,  5.17227091e-01],\n",
      "       [-5.97758817e-01, -2.97853259e-01, -1.56916492e+00,\n",
      "        -1.05979924e+00, -2.06778105e-01, -8.80281314e-01,\n",
      "        -6.06407009e-01,  4.03795218e-01, -1.33481622e-01,\n",
      "        -1.87929068e-01,  2.48080244e-01,  6.87178815e-01],\n",
      "       [-1.65322325e+00,  4.11158541e-01, -5.63536441e-01,\n",
      "         3.09909353e-01,  1.08361200e+00,  1.68641326e+00,\n",
      "        -5.03726244e-01, -1.53554383e-01, -1.01618479e+00,\n",
      "        -1.58433554e+00,  4.56929322e-01,  1.70527121e+00],\n",
      "       [ 2.60959159e-01, -1.25150216e+00,  1.49092351e-02,\n",
      "         8.93119473e-01, -3.09767317e-02,  1.06341032e+00,\n",
      "         2.22164153e-01,  8.46448624e-01, -5.93817224e-01,\n",
      "         1.26151102e+00,  4.42594951e-01,  3.11240822e-01],\n",
      "       [-3.22945303e+00, -1.56956455e+00, -6.71074691e-01,\n",
      "         7.11144121e-01,  2.00053262e-01, -4.21051720e-01,\n",
      "        -1.30555764e+00,  8.88315258e-01, -8.56561067e-01,\n",
      "        -2.19083776e+00,  6.50504691e-01, -1.73761440e+00],\n",
      "       [-2.27036770e-01,  4.76672797e-01,  1.02970087e+00,\n",
      "         7.95356824e-01,  2.60955961e-01,  6.27780123e-01,\n",
      "         3.83625901e-01,  2.70593159e-01, -1.10205592e+00,\n",
      "         7.16789660e-01, -4.46449048e-01, -1.04623209e+00],\n",
      "       [-1.44621615e+00, -1.53132107e-01,  2.53852467e+00,\n",
      "         8.36825924e-01,  3.46111464e-01, -1.16571706e+00,\n",
      "        -1.21374434e+00, -8.21841864e-01, -7.65918638e-01,\n",
      "         2.78081824e-02, -5.77104266e-01,  9.45254722e-02],\n",
      "       [-8.99592230e-01,  1.54720568e-04,  1.71388668e-01,\n",
      "        -6.18495722e-01, -9.20393304e-01,  1.01210215e+00,\n",
      "        -8.19266271e-01,  7.97110974e-01, -2.00332193e+00,\n",
      "        -1.78715386e+00,  1.09760170e+00, -1.11653001e+00],\n",
      "       [-1.56748091e+00,  3.84356334e-01, -1.04133670e+00,\n",
      "         1.86227482e-01,  3.21213206e-02, -5.56371289e-01,\n",
      "         1.09166457e+00,  1.23609140e+00,  3.40805126e-02,\n",
      "        -4.67924585e-01,  6.41724333e-01, -5.80086183e-01],\n",
      "       [-1.06633861e+00, -1.27033731e+00, -1.07231295e+00,\n",
      "         2.86057107e-01, -4.43319097e-01,  3.57711254e-01,\n",
      "        -1.56268510e+00, -4.14473697e-01,  8.88431212e-01,\n",
      "         1.51761800e+00, -3.21830492e-01, -3.68355701e-01],\n",
      "       [ 1.06538259e+00, -2.02413842e+00, -4.22045691e-02,\n",
      "        -1.07449880e+00,  7.99687197e-03, -1.18194950e+00,\n",
      "        -1.28285713e+00, -8.28029427e-01, -4.12515532e-01,\n",
      "        -2.67618885e+00,  7.57007432e-01, -5.87614707e-01],\n",
      "       [ 4.28500718e-02, -1.98861559e+00,  5.49932350e-01,\n",
      "         3.84394998e-02,  4.28398002e-01,  3.43820491e-02,\n",
      "         5.60479289e-01,  2.75656154e-01, -3.26085205e-01,\n",
      "         8.24608097e-01,  3.41453634e-01,  5.50403519e-02],\n",
      "       [ 1.19781472e+00,  1.71613098e-02,  1.04274032e+00,\n",
      "        -1.31866830e+00, -1.85537721e-02,  1.87680434e+00,\n",
      "        -1.14528699e+00,  4.82068146e-01, -2.89405718e+00,\n",
      "        -6.80660165e-02, -1.01762759e+00,  1.92391873e-01]]), array([[-0.19324711, -0.33681838, -0.41782346, -1.00140086, -0.3543564 ,\n",
      "         1.00501529,  1.03164012,  0.14294354, -0.69034204, -0.49807755],\n",
      "       [-0.5016903 ,  0.47346611, -0.50154866,  2.12582926,  0.13883921,\n",
      "         0.80359143,  1.01354207, -1.56138914,  0.60257574, -0.48448526],\n",
      "       [ 0.67106795, -0.07411188,  0.40582626,  0.58156783,  1.6313567 ,\n",
      "        -0.19922112,  0.24573844, -0.61750776,  2.02364555,  0.35793759],\n",
      "       [-0.01047223, -0.05436169,  0.39958026,  0.41146865, -0.39609317,\n",
      "         0.61141943,  2.01213033, -0.86669397, -0.60997825, -0.72019261],\n",
      "       [ 0.03294237,  0.1412614 , -1.47748181,  1.57684423, -0.4325711 ,\n",
      "         1.29652037, -1.7184825 , -0.50617781, -0.43396741,  1.82253382],\n",
      "       [ 0.57836599,  0.15981228,  1.02384859, -0.13874734, -1.98837482,\n",
      "        -0.818545  ,  1.35728068, -0.25357326,  0.97786497,  0.07964943],\n",
      "       [-0.84243321, -0.64525526, -1.53364704, -0.61996083,  0.52523227,\n",
      "        -1.0517527 ,  0.22092423,  0.02684473, -0.58719594,  0.65952924],\n",
      "       [-0.39110438, -1.17955819,  1.49669595, -0.45266254,  0.49549843,\n",
      "         1.32601461, -0.34969282, -0.35627269, -0.10207006,  0.03092797],\n",
      "       [-1.47816922, -0.65322697,  0.0700806 ,  0.60506086, -0.07033842,\n",
      "         0.27202077, -0.08900665, -0.91773929, -0.06462732, -0.40957698],\n",
      "       [-0.20716676, -0.12795016,  2.35883998, -1.41567118,  0.66814972,\n",
      "        -0.86470755,  1.64860093,  0.36602625,  1.54517816, -0.94967605],\n",
      "       [ 2.80177023,  0.77350909,  0.5210007 , -0.28624846,  0.24210574,\n",
      "        -0.8883061 , -0.53732375, -0.71682765,  0.02835363, -0.49877653],\n",
      "       [-1.21444231, -0.63891038, -1.55954968,  2.62561252,  1.01922052,\n",
      "         0.65081694, -1.41280392, -0.04627804,  1.49444393, -0.39850024]])]\n",
      "Biases: [array([-0.2317876 ,  0.3188853 ,  1.75975466, -0.69379538,  0.04099104,\n",
      "        0.49614726,  0.30698255,  0.0471202 , -1.10620313,  1.07731409,\n",
      "       -0.05616895,  0.45538847, -1.78015883,  0.49901064,  1.28356986,\n",
      "       -1.01825744, -0.82604561,  0.1009474 ,  0.91422146, -1.48412367,\n",
      "       -2.05396138,  1.1017438 ,  1.60044611,  1.12636261]), array([ 1.57368636, -0.22338502,  0.70264504,  1.3382741 ,  0.3216137 ,\n",
      "       -0.26488011, -0.10555779,  0.13357819, -2.31366663,  0.43721199,\n",
      "       -0.87633991, -0.56513771]), array([ 0.84999505,  2.14451511,  1.23228875, -0.16284121,  0.14986668,\n",
      "        1.30590874, -0.15796   , -0.24113821, -0.41781209, -0.46953546])]\n"
     ]
    }
   ],
   "source": [
    "# Initialize weights and biases for each layer\n",
    "np.random.seed(40)  # for reproducibility\n",
    "\n",
    "w = []\n",
    "for i in range(len(neuron_layers) - 1):\n",
    "    # Matrix of weights for Layer i to Layer i+1\n",
    "    weights = np.random.randn(neuron_layers[i], neuron_layers[i + 1]) # matrix of neuronal_layers[i] rows and neuronal_layers[i+1] columns\n",
    "    w.append(weights)\n",
    "\n",
    "b = []\n",
    "for i in range(1, len(neuron_layers)):\n",
    "    # Bias vector for Layer i\n",
    "    biases = np.random.randn(neuron_layers[i]) # vector of neuronal_layers[i] entries\n",
    "    b.append(biases)\n",
    "\n",
    "print(\"Weights:\", w)\n",
    "print(\"Biases:\", b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4b39b8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4f94b7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feed_forward_neural_network(neuron_vals, w, b):\n",
    "    activations = [neuron_vals]\n",
    "\n",
    "    for layer_weights, layer_biases in zip(w, b):\n",
    "        neuron_vals_before_activation = neuron_vals @ layer_weights + layer_biases # first matrix multiplication, then + bias\n",
    "\n",
    "        if layer_weights is not w[-1]:\n",
    "            # we only want to apply the activation function to the hidden layers, not the output layer\n",
    "            neuron_vals = sigmoid(neuron_vals_before_activation)\n",
    "        else:\n",
    "            neuron_vals = neuron_vals_before_activation  # output layer without activation function\n",
    "\n",
    "        ######## ------------- WHY NO ACTIVATION FUNCTION IN THE OUTPUT LAYER? ------------- ########\n",
    "        # !!! here was a mistake before, I applied the sigmoid function to the output layer as well\n",
    "        # for multi-class predictions we don't want to do that tho, because then the softmax function produces unsatisfying results\n",
    "        # the values after the softmax function are not as \"confident\" and are pretty flat\n",
    "        # the softmax function can also handle negative values, which we would not have if we applied the sigmoid function before\n",
    "        # e.g. if the values are really small after the sigmoid (close to 0), the softmax would go close to (1/number_of_classes) for the wrong classes and also flattening the correct class, which is not what we want\n",
    "\n",
    "        activations.append(neuron_vals)\n",
    "        \n",
    "    return activations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5e16b96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    output = np.exp(z) / np.sum(np.exp(z), axis=1, keepdims=True) # axis=1 means sum each row, keepdims=True ensure the result is (n, 1) and not (n,)\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a2fbd12b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.18865108235383574\n"
     ]
    }
   ],
   "source": [
    "def calc_log_loss(features, labels, w, b):\n",
    "    pred_label = feed_forward_neural_network(features, w, b)[-1]\n",
    "    pred_label = softmax(pred_label)\n",
    "\n",
    "    pred_label = np.clip(pred_label, 1e-10, 1 - 1e-10)\n",
    "    log_loss = np.mean(np.sum(-labels * np.log(pred_label), axis=1))\n",
    "    # using categorical cross-entropy loss function, because we have multiple classes (10 digits)\n",
    "    # otherwise, when we would use binary cross-entropy loss function, the problem would push other values more and more to zero => softmax function already does that\n",
    "    # (probabilities for other classes automatically go lower when the correct class probability goes higher)\n",
    "    return log_loss\n",
    "\n",
    "log_loss = calc_log_loss(train_X, train_Y, w, b)\n",
    "print(f\"Loss: {log_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4608e9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_gradients(features, labels, w, b):\n",
    "    dB = [np.zeros_like(bias) for bias in b]  # gradient for biases\n",
    "    dW = [np.zeros_like(weights) for weights in w]  # gradient for weights\n",
    "\n",
    "    activations = feed_forward_neural_network(features, w, b)\n",
    "    predictions = softmax(activations[-1])  # apply softmax to the output layer activations\n",
    "    \n",
    "    delta = predictions - labels    # initial delta (dL/da)\n",
    "    ####### ------------- WHY CAN WE JUST WRITE THE dL/dz FUNCTION LIKE THIS? ------------- #######\n",
    "    # when using softmax + categorical cross-entropy loss function, the derivative of the loss with respect to the z-function (dL/dz) is simply (predictions - labels)\n",
    "    # they cancel each other out, simplifying like this\n",
    "\n",
    "    for idx in range(len(w)-1, -1, -1):\n",
    "        if (idx == len(w)-1):\n",
    "            pass\n",
    "        else:\n",
    "            layer_activations = activations[idx+1].T    # current layer activations to calculate the sigmoid derivative\n",
    "            der_sigmoid = layer_activations * (1 - layer_activations)   # derivative of sigmoid\n",
    "\n",
    "\n",
    "            following_weights = w[idx + 1]  # weights of the next layer to calculate the derivative of the z-function (dz/da) with respect to the activations\n",
    "\n",
    "            delta = delta @ following_weights.T * der_sigmoid.T  # update delta for next layer (dz/da * da/dz)\n",
    "    \n",
    "        der_weights = activations[idx].T  # previous layer activations to calculate the z-derivative (dz/dw)\n",
    "\n",
    "        dW[idx] = der_weights @ delta / features.shape[0]  # gradient for weights (dz/dw)  => shape[0] is the number of rows (num samples)\n",
    "        # we have multiple der_weights arrays (one for each previous sample), so for every previous neuron we multiply take the mean from all samples for every of the previous neurons\n",
    "        # dW has the amount of previous neurons rows and the amount of current neurons columns\n",
    "        # (first row = first previous neuron, first column = first current neuron, second column = second current neuron, ...)\n",
    "        \n",
    "        dB[idx] = delta.mean(axis=0) # gradient for bias (dz/db)\n",
    "    return dW, dB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f46e5e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_weights_and_biases(w, b, dW, dB, learning_rate):\n",
    "    #print(\"Weights before update:\", w)\n",
    "    #print(\"Weight Gradients:\", dW)\n",
    "\n",
    "    for i in range(len(w)):\n",
    "        w[i] -= learning_rate * dW[i]\n",
    "        b[i] -= learning_rate * dB[i]\n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "04e85315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Loss: 3.6444450206941554\n",
      "Epoch 0, Loss: 2.9403115374149915\n",
      "Epoch 100, Loss: 1.0926909241170932\n",
      "Epoch 200, Loss: 0.8132193850557378\n",
      "Epoch 300, Loss: 0.6624373259762593\n",
      "Epoch 400, Loss: 0.5706155963755651\n",
      "Epoch 500, Loss: 0.5085612416384934\n",
      "Epoch 600, Loss: 0.4632590196741711\n",
      "Epoch 700, Loss: 0.42944309950892645\n",
      "Epoch 800, Loss: 0.4034237287193419\n",
      "Epoch 900, Loss: 0.3824802309162488\n",
      "Epoch 1000, Loss: 0.36490496133874295\n",
      "Epoch 1100, Loss: 0.35010212133285246\n",
      "Epoch 1200, Loss: 0.3374790472678998\n",
      "Epoch 1300, Loss: 0.32649011660257043\n",
      "Epoch 1400, Loss: 0.3167885815776579\n",
      "Epoch 1500, Loss: 0.30813116773664234\n",
      "Epoch 1600, Loss: 0.3003247727482681\n",
      "Epoch 1700, Loss: 0.29322063046241637\n",
      "Epoch 1800, Loss: 0.2867018245638078\n",
      "Epoch 1900, Loss: 0.28068421913003294\n",
      "Epoch 2000, Loss: 0.2751084952938169\n",
      "Epoch 2100, Loss: 0.26992630125253914\n",
      "Epoch 2200, Loss: 0.2650890600724254\n",
      "Epoch 2300, Loss: 0.26055038546731646\n",
      "Epoch 2400, Loss: 0.2562728484447042\n",
      "Epoch 2500, Loss: 0.2522262829712741\n",
      "Epoch 2600, Loss: 0.24838546718085674\n",
      "Epoch 2700, Loss: 0.2447278592329921\n",
      "Epoch 2800, Loss: 0.24123434222317294\n",
      "Epoch 2900, Loss: 0.23789013185480956\n",
      "Epoch 3000, Loss: 0.23468402433516106\n",
      "Epoch 3100, Loss: 0.23160139525492235\n",
      "Epoch 3200, Loss: 0.22862552324202684\n",
      "Epoch 3300, Loss: 0.22574573773486528\n",
      "Epoch 3400, Loss: 0.22295954568956314\n",
      "Epoch 3500, Loss: 0.22026877825159058\n",
      "Epoch 3600, Loss: 0.2176741058770607\n",
      "Epoch 3700, Loss: 0.21517263158533173\n",
      "Epoch 3800, Loss: 0.21275866507153696\n",
      "Epoch 3900, Loss: 0.2104254499042577\n",
      "Epoch 4000, Loss: 0.20816616857274306\n",
      "Epoch 4100, Loss: 0.20597427571596966\n",
      "Epoch 4200, Loss: 0.2038435683308263\n",
      "Epoch 4300, Loss: 0.2017682967944322\n",
      "Epoch 4400, Loss: 0.19974361659130335\n",
      "Epoch 4500, Loss: 0.19776624822578015\n",
      "Epoch 4600, Loss: 0.19583534990209056\n",
      "Epoch 4700, Loss: 0.1939541429134911\n",
      "Epoch 4800, Loss: 0.19212783698542382\n",
      "Epoch 4900, Loss: 0.1903561653568928\n"
     ]
    }
   ],
   "source": [
    "def train_network(features, labels, w, b):\n",
    "    initial_loss = calc_log_loss(features, labels, w, b)\n",
    "    print(\"Initial Loss:\", initial_loss)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        dW, dB = calc_gradients(features, labels, w, b)\n",
    "        w, b = update_weights_and_biases(w, b, dW, dB, learning_rate)\n",
    "\n",
    "        if epoch % 100 == 0:\n",
    "            loss = calc_log_loss(features, labels, w, b)\n",
    "            print(f\"Epoch {epoch}, Loss: {loss}\")\n",
    "\n",
    "\n",
    "train_network(train_X, train_Y, w, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "44f15633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.41116952e-06 1.91342008e-05 2.76511910e-04 ... 9.98099428e-01\n",
      "  2.18744598e-05 3.35846481e-04]\n",
      " [1.18952690e-04 1.98544280e-02 9.73348359e-01 ... 2.00468137e-04\n",
      "  2.74496388e-04 4.72229069e-07]\n",
      " [8.92744844e-07 9.95611822e-01 1.01115749e-04 ... 6.20551628e-05\n",
      "  1.70410594e-03 9.51752202e-07]\n",
      " ...\n",
      " [5.75267486e-07 6.05249690e-06 2.06609501e-05 ... 4.09622110e-04\n",
      "  9.35202920e-04 1.17498031e-02]\n",
      " [1.16085413e-04 2.89354982e-03 2.82498804e-06 ... 6.56697836e-04\n",
      "  1.75402542e-03 1.55472502e-04]\n",
      " [2.74773056e-05 9.67928960e-06 3.69314259e-04 ... 4.50352629e-06\n",
      "  1.33333598e-05 1.26177507e-07]]\n",
      "Accuracy: 93.22%\n"
     ]
    }
   ],
   "source": [
    "def predict(test_X, test_Y, w, b):\n",
    "    activations = feed_forward_neural_network(test_X, w, b)\n",
    "    predictions = activations[-1]\n",
    "    predictions = softmax(predictions)\n",
    "    print(predictions)\n",
    "\n",
    "    predicted_classes = np.argmax(predictions, axis=1)\n",
    "    true_classes = np.argmax(test_Y, axis=1)\n",
    "\n",
    "    accuracy = np.mean(predicted_classes == true_classes)\n",
    "    print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "predict(test_X, test_Y, w, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14897ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save weights and biases\n",
    "weight_dict = {f'weights_{i}': w[i] for i in range(len(w))}\n",
    "bias_dict = {f'biases_{i}': b[i] for i in range(len(b))}\n",
    "\n",
    "file_name = \"mnist_model_v1_93_22.npz\"\n",
    "\n",
    "np.savez(file_name, **weight_dict, **bias_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1b1c0fc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: 9\n"
     ]
    }
   ],
   "source": [
    "# load weights and biases from file\n",
    "file_name = \"mnist_model_v1_93_22.npz\"\n",
    "data = np.load(file_name)\n",
    "w = [data[f'weights_{i}'] for i in range(len(w))]\n",
    "b = [data[f'biases_{i}'] for i in range(len(b))]\n",
    "\n",
    "# plot a picture of a number\n",
    "def plot_number(index):\n",
    "    image = test_X[index].reshape(28, 28)  # reshape the flat array to a 28x28 matrix\n",
    "    plt.imshow(image, cmap='gray')\n",
    "    plt.title(f\"Label: {test_Y_original[index]}\")\n",
    "    plt.axis('off')  # turn off axis labels\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def get_number_prediction(index):\n",
    "    image = test_X[index:index+1]\n",
    "    activations = feed_forward_neural_network(image, w, b)\n",
    "    predictions = softmax(activations[-1])\n",
    "\n",
    "    predicted_class = np.argmax(predictions, axis=1)[0]\n",
    "    print(f\"Predicted class: {predicted_class}\")\n",
    "\n",
    "    return predicted_class\n",
    "\n",
    "def predict_index(index=0):\n",
    "    plot_number(index)\n",
    "    get_number_prediction(index)\n",
    "\n",
    "predict_index(12)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuronales-netz",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
